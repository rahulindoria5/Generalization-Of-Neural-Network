{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "np.seterr(all='ignore') # ignore numpy warning\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import scipy.special # get sigmoid function\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib qt \n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools \n",
    "import collections\n",
    "from batchup import data_source # create batches from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:    \n",
    "    #initialize the neural network\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initialization of neural networks\n",
    "        \"\"\"\n",
    "        self.load_dataset()\n",
    "        self.neural_network_functions()\n",
    "        pass\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        load and split the data into train, test and validation set\n",
    "        \"\"\"\n",
    "        iris = datasets.load_iris() # load dataset\n",
    "\n",
    "        # Load the targets and convert them to \n",
    "        # one-hot-encoding for the output softmax layer.\n",
    "        self.T = np.zeros((iris.target.shape[0],3))\n",
    "        self.T[np.arange(len(self.T)), iris.target] += 1\n",
    "                                \n",
    "        # Divide the data into a train and test set.\n",
    "        self.X_train, self.X_test, self.T_train, self.T_test = cross_validation.train_test_split(iris.data,\n",
    "                                                                                                 self.T,\n",
    "                                                                                                 test_size=0.3,\n",
    "                                                                                                 random_state = 10)\n",
    "        \n",
    "        # Divide the test set into a validation set and final test set.\n",
    "        self.X_validation, self.XV_test, self.T_validation, self.TV_test = cross_validation.train_test_split(self.X_test,\n",
    "                                                                                                             self.T_test,\n",
    "                                                                                                             test_size=.5)\n",
    "        pass\n",
    "    \n",
    "    def neural_network_functions(self):        \n",
    "        \"\"\"\n",
    "        it contains non-linear functions:\n",
    "        1.) activation functions: sigmoid, softmax, \n",
    "        2.) cost function: cross_entropy\n",
    "        3.) gradients : sigmoid, \n",
    "        4.) learning rule: weight update(delta_w)\n",
    "        \"\"\"        \n",
    "        #sigmoid\n",
    "        self.sigmoid = lambda x: scipy.special.expit(x)                \n",
    "        #softmax\n",
    "        self.softmax = lambda z : np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "        #cross_entropy or cost function\n",
    "        self.cross_entropy_loss = lambda y, t: - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))    \n",
    "        #gradients\n",
    "        self.gradient = lambda w, x, t: (self.nn(x, w) - t).T * x\n",
    "        #sigmoid derivative                \n",
    "        self.sigmoid_gradient = lambda y: np.multiply(y, (1 - y))\n",
    "        #update delta w\n",
    "        #self.delta_w = lambda w_k, x, t, learning_rate: learning_rate * self.gradient(w_k, x, t)                \n",
    "        pass    \n",
    "    \n",
    "       \n",
    "    #Training of model involves two steps:\n",
    "    #1.) forward propagation\n",
    "    #2.) backward propagation\n",
    "    def forward_propagation_step(self, inputs, list_of_layer):\n",
    "        \"\"\"\n",
    "        Forward propagation: it is used to calculate the activations(or outputs) of all layers including \n",
    "        the inputs to the layer and stores them in a list \n",
    "        \"\"\"\n",
    "        n_layer_activations = [inputs]    # initialize the list with input\n",
    "        x = inputs                        # initialize variable \"x\" with inputs\n",
    "        for nth_layer in list_of_layer:   # access each layer iteratively \n",
    "            y = nth_layer.get_output(x)   # get output from nth-layer\n",
    "            n_layer_activations.append(y) # save nth-layer output in activation list        \n",
    "            x = n_layer_activations[-1]   # get the last layer activation values and make it input for next layer\n",
    "            pass        \n",
    "        return n_layer_activations        # return a list of activations of every layer\n",
    "\n",
    "    def back_propagation_step(self, list_of_activations, targets, list_of_layer):\n",
    "        \"\"\"\n",
    "        Backpropagation step:\n",
    "        it involves updating the parameter in the model        \n",
    "        \"\"\"\n",
    "        param_grads = collections.deque()\n",
    "        output_grad = None\n",
    "\n",
    "        for nth_last_layer in reversed(list_of_layer): # access every layer from the last layer\n",
    "            y = list_of_activations.pop()              # get activations from last layer and delete the \n",
    "                                                       # last layer from the list\n",
    "            if output_grad is None:                    # if there is no gradient in the output layer\n",
    "                input_grad = nth_last_layer.get_input_grad(y, targets)\n",
    "                pass\n",
    "            else :\n",
    "                input_grad = nth_last_layer.get_input_grad(y, output_grad)\n",
    "                pass\n",
    "\n",
    "            x = list_of_activations[-1]                            # get activation from the last layer \n",
    "            grads = nth_last_layer.get_params_grad(x, output_grad) # get parameters of gradient \n",
    "                                                                   # between nth and (n-1)th layer.\n",
    "                                                                   # these are change weights and bias\n",
    "            param_grads.appendleft(grads)                          # make a list of parameter gradients\n",
    "            \n",
    "            output_grad = input_grad\n",
    "            \n",
    "        return list(param_grads) # return list of parameters\n",
    "    \n",
    "    # Define a method to update the parameters(or  weights)\n",
    "    def delta_weight(self, list_of_layer, param_grads, learning_rates):\n",
    "        \"\"\"\n",
    "        Function to update the parameters of the given layers with the given gradients\n",
    "        by gradient descent with the given learning rate.      \n",
    "        \"\"\"     \n",
    "        for layer, layer_backprop_grads in zip(list_of_layer, param_grads):        \n",
    "            for param, grad in zip(layer.get_params_iter(), layer_backprop_grads): #access each parameter and gradient\n",
    "                param -= learning_rates * grad # Update each parameter using gradient descent\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def train(self, list_of_layer, X_train, T_train, epochs, learning_rates):        \n",
    "        \"\"\"\n",
    "        Perform training of data using forward and back propagation functions \n",
    "        given above and store the training and validation costs in every epoch\n",
    "        for future analsis.\n",
    "        \"\"\"        \n",
    "        training_costs = []   # initialize list to store the costs of training in every epoch\n",
    "        validation_costs = [] # initialize list to store the costs of validation in every epoch\n",
    "        \n",
    "        # Construct an array of data source\n",
    "        ds = data_source.ArrayDataSource([X_train, T_train]) # use training data and its target labels                \n",
    "        \n",
    "        print (\"Waiting for the model to be trained ...\")        \n",
    "        for epoch in range(epochs): # Training with maximum number of epochs\n",
    "            # Iterate over samples and use batches of 25 samples in a random order\n",
    "            for X_train_data, T_label in ds.batch_iterator(batch_size=25, shuffle=np.random.RandomState(25)): \n",
    "                # get a list of activations from all layers\n",
    "                activations = self.forward_propagation_step(X_train_data,     # sample of batch\n",
    "                                                            list_of_layer)    # list of layer                \n",
    "                # get list of parameter gradients\n",
    "                param_grads = self.back_propagation_step(activations,         # activations from forward propagation\n",
    "                                                         T_label,             # labels of samples in a batch\n",
    "                                                         list_of_layer)       # list of layer                \n",
    "                # Update parameters(or weights) using gradient descent\n",
    "                self.delta_weight(list_of_layer,  # list of layer\n",
    "                                  param_grads,    # parameter gradients\n",
    "                                  learning_rates) # learning rate\n",
    "\n",
    "            # Get full training cost in a list\n",
    "            # get activations(or output) of every layer in a list\n",
    "            activations = self.forward_propagation_step(X_train,         # training data\n",
    "                                                        list_of_layer)   # list of layer\n",
    "            # get training cost at the last layer\n",
    "            train_cost = list_of_layer[-1].get_cost(activations[-1],     # output from last layer\n",
    "                                                    T_train)             # training label at the output\n",
    "            # append list of training cost for every epoch\n",
    "            training_costs.append(train_cost)                            \n",
    "            \n",
    "            # Get full validation cost in a list\n",
    "            # get activations(or output) of every layer in a list from validation data\n",
    "            activations = self.forward_propagation_step(self.X_validation, # validation data\n",
    "                                                        list_of_layer)     # list of layer\n",
    "            # get validation cost at output layer\n",
    "            validation_cost = list_of_layer[-1].get_cost(activations[-1],   # output from last layer\n",
    "                                                         self.T_validation) # validation label at the output\n",
    "            # append list of validation cost for every epoch\n",
    "            validation_costs.append(validation_cost) \n",
    "\n",
    "            \n",
    "            if len(validation_costs) > 3: # Stop training if the cost on the validation \n",
    "                                          # set doesn't decrease for 3 iterations\n",
    "                if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "                    break\n",
    "\n",
    "        #nb_of_epochs = epoch + 1  # total number of epochs that have been executed\n",
    "        print (\"Model trained !\")\n",
    "        return training_costs\n",
    "\n",
    "    def test_accuracy(self, list_of_layer, X_test, T_test):   \n",
    "        \"\"\"\n",
    "        Get the test accuracy of the test data on the trained model\n",
    "        \"\"\"\n",
    "        y_true = np.argmax(T_test, axis=1)                                  # Get the target outputs\n",
    "        activations = self.forward_propagation_step(X_test, list_of_layer)  # Get activation of test samples\n",
    "        y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "        test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Get test set accuracy\n",
    "        print('The accuracy on the test set is {:.2f}'.format(test_accuracy))\n",
    "        pass\n",
    "    \n",
    "    def plot_cost_vs_epochs(self, nb_of_epochs, training_costs):\n",
    "        \"\"\"\n",
    "        Evaluate the performce of the model by plotting the graph between \n",
    "        cost and epochs\n",
    "        \"\"\"\n",
    "        # Plot full training costs\n",
    "        epoch_x_inds = np.linspace(1, nb_of_epochs, num=nb_of_epochs)\n",
    "        # Plot the cost over the iterations\n",
    "        plt.plot(epoch_x_inds, training_costs, 'r-', linewidth=3, label='cost full training set')\n",
    "        # Add labels to the plot\n",
    "        plt.xlabel('Epochs --->')\n",
    "        plt.ylabel('$Error --->$', fontsize=15)\n",
    "        plt.title('cost vs epochs')\n",
    "        plt.legend()\n",
    "        x1,x2,y1,y2 = plt.axis()\n",
    "        plt.axis((0,nb_of_epochs,0,2.5))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our application we need three different kinds of layers:\n",
    "# 1.) Linear layer\n",
    "# 2.) Sigmoid/Logistic layer\n",
    "# 3.) Softmax layer\n",
    "# for these three layers we need a base class, which\n",
    "# can share some function among the three given layer\n",
    "\n",
    "\n",
    "# Base class with common methods among the layers\n",
    "# used in the model\n",
    "class BaseLayer(object):\n",
    "    \"\"\"\n",
    "    Base class for three different layers:\n",
    "    This class contains some basic methods which \n",
    "    are used among the three different types of layers \n",
    "    used in the given model of neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"\n",
    "        Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"\n",
    "        Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        1.) \"X\" is the input.\n",
    "        2.) \"output_grad\" is the gradient at the output of the given layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        performs linear transformation in the feed forward step.\n",
    "        and it will act as an input to the next layer(or to the\n",
    "        activation layer).\n",
    "        1.) \"X\" is the input.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"\n",
    "        During backpropagation, it returns the gradient\n",
    "        of the input of the given given layer.        \n",
    "        1.) \"Y\" is the pre-computed output of the given layer\n",
    "        during forward propagation.         \n",
    "        2.) For the output layer instead of using \"Y\", output\n",
    "        layer uses the Target \"T\" to compute the gradient \n",
    "        instead of using the output_grad.        \n",
    "        3.) \"output_grad\" is the gradient at the output of the given layer\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    \"Linear Layer\" performs a linear transformation to its input.\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"    \n",
    "    def __init__(self, \n",
    "                 no_of_input_nodes, \n",
    "                 no_of_output_nodes):\n",
    "        # linear layers initializes the weights and bias \n",
    "        # between the two consecutive layers whereas as rest\n",
    "        # of the layers are responsible for the computation \n",
    "        # of non-linear (or activation functions) functions.        \n",
    "        self.W = np.random.randn(no_of_input_nodes, # number of input variables\n",
    "                                 no_of_output_nodes # number of output variables\n",
    "                                ) * 0.1            \n",
    "        self.b = np.zeros(no_of_output_nodes) # bias initialized with zero\n",
    "        pass\n",
    "        \n",
    "    # forward propagation step\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Performs the linear transformation for the forward \n",
    "        step which will act as an input to the activation layer.\n",
    "        \"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "    \n",
    "    # forward propagation step or used for paramater update\n",
    "    def get_params_iter(self):\n",
    "        \"\"\"\n",
    "        Return an iterator over the parameters.\n",
    "        \"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"\n",
    "        During the backpropagation it returns a list \n",
    "        of gradients over the parameters(or weight and\n",
    "        parameters)\n",
    "        \"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"\n",
    "        Returns the gradient(or error) at the inputs of\n",
    "        the given layer.\n",
    "        \"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic layer\n",
    "class SigmoidLayer(BaseLayer, NeuralNetwork):\n",
    "    \"\"\"\n",
    "    \"Logistic Layer\": computes the logistic or sigmoid\n",
    "    function from the output of the \"Linear Layer\"\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward propagation step:\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward step transformation.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(X)\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"\n",
    "        Return the gradient at the inputs of this layer.\n",
    "        \"\"\"\n",
    "        return np.multiply(self.sigmoid_gradient(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax layer\n",
    "class SoftmaxLayer(BaseLayer, NeuralNetwork):\n",
    "    \"\"\"\n",
    "    The softmax output layer computes the classification propabilities at the output.\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"\n",
    "    #forward propagation\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward step transformation.\n",
    "        \"\"\"\n",
    "        return self.softmax(X)\n",
    "    \n",
    "    #backward propagation\n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"\n",
    "        Return the gradient at the inputs of this layer.\n",
    "        \"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    # for testing the network on the large training data\n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"\n",
    "        Cross-Entropy cost function\n",
    "        Return the cost at the output of this output layer.\n",
    "        \"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for the model to be trained ...\n",
      "Model trained !\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.96\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    \"\"\"\n",
    "    Create and train a model\n",
    "    sample of model with layers is :\n",
    "    input layer -> linear layer -> sigmoid layer-> linear layer -> sigmoid layer -> linear layer -> softmax layer\n",
    "    \"\"\"\n",
    "    # instantiate neural network class\n",
    "    ANN = NeuralNetwork()\n",
    "    \n",
    "    # add nodes(or neurons) in the hidden layer,\n",
    "    no_of_hidden_nodes_layer_1 = 25 # hidden layer 1  \n",
    "    no_of_hidden_nodes_layer_2 = 15 # hidden layer 2  (not required)\n",
    "    no_of_hidden_nodes_layer_3 = 20 # hidden layer 3  (not required)\n",
    "    \n",
    "    list_of_layers = [] # store layers in a list\n",
    "    \n",
    "    # please note: For our architecutre \n",
    "    # one hidden layer is a combination of \"Linear layer + sigmoid layer\"\n",
    "    # and output layer is a combination of \"Linear layer + softmax layer\"\n",
    "    \n",
    "    # Add first hidden layer: \n",
    "    list_of_layers.append(LinearLayer                   # add \"Linear layer\" and arguments are :\n",
    "                         (ANN.X_train.shape[1],         # 1.) number of input nodes\n",
    "                          no_of_hidden_nodes_layer_1))  # 2.) number of hidden nodes\n",
    "    list_of_layers.append(SigmoidLayer())               # add \"Sigmoid layer\"\n",
    "    \n",
    "    # Add second hidden layer\n",
    "    # list_of_layers.append(LinearLayer\n",
    "                          # (no_of_hidden_nodes_layer_1, \n",
    "                          # no_of_hidden_nodes_layer_2))\n",
    "    # list_of_layers.append(SigmoidLayer())    \n",
    "    \"\"\"\n",
    "    Here you can add \"N\" number of layers exactly in the same way as above......\n",
    "    Examples are :\n",
    "    # list_of_layers.append(LinearLayer\n",
    "                            (no_of_nodes_in_nth_hidden_layer, \n",
    "                             no_of_nodes_in_nth+1_hidden_layer))\n",
    "    # list_of_layers.append(SigmoidLayer())                                        \n",
    "    \"\"\"\n",
    "    # Add output layer\n",
    "    list_of_layers.append(LinearLayer                   # add \"Linear Layer\" and arguments are:\n",
    "                         (no_of_hidden_nodes_layer_1,   # 1.) number of input nodes\n",
    "                          ANN.T_train.shape[1]))        # 2.) number of output nodes    \n",
    "    list_of_layers.append(SoftmaxLayer())               # add softmax layer\n",
    "    # model created upto this point\n",
    "        \n",
    "        \n",
    "    # After model creation, train the model\n",
    "    # and get training costs\n",
    "    no_of_epochs = 500    # set number of epochs for training\n",
    "    learning_rate = 0.15  # set learning rate for training\n",
    "    \n",
    "    training_costs = ANN.train(list_of_layers, # layers stored in a list \n",
    "                               ANN.X_train,    # training data \n",
    "                               ANN.T_train,    # training labels\n",
    "                               no_of_epochs,   # number of epochs \n",
    "                               learning_rate)  # learning rate\n",
    "        \n",
    "    # plot the graph between the training costs and the number of epochs        \n",
    "    ANN.plot_cost_vs_epochs(no_of_epochs,   # number of epochs\n",
    "                            training_costs) # list of training costs in every epoch\n",
    "    \n",
    "    # test accuracy of the network\n",
    "    ANN.test_accuracy(list_of_layers,  # layers stored in a list\n",
    "                      ANN.XV_test,     # validation test data\n",
    "                      ANN.TV_test)     # validation test training labels\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
