{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "np.seterr(all='ignore') # ignore numpy warning\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import scipy.special # get sigmoid function\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib qt \n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools \n",
    "import collections\n",
    "from batchup import data_source # create batches from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:    \n",
    "    #initialize the neural network\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initialization of neural networks\n",
    "        \"\"\"\n",
    "        self.load_dataset()\n",
    "        self.neural_network_functions()\n",
    "        pass\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        load and split the data into train, test and validation set\n",
    "        \"\"\"\n",
    "        iris = datasets.load_iris() # load dataset\n",
    "\n",
    "        # Load the targets and convert them to \n",
    "        # one-hot-encoding for the output softmax layer.\n",
    "        self.T = np.zeros((iris.target.shape[0],3))\n",
    "        self.T[np.arange(len(self.T)), iris.target] += 1\n",
    "        \n",
    "                        \n",
    "        # Divide the data into a train and test set.\n",
    "        self.X_train, self.X_test, self.T_train, self.T_test = cross_validation.train_test_split(iris.data,\n",
    "                                                                                                 self.T,\n",
    "                                                                                                 test_size=0.3, \n",
    "                                                                                                 random_state = 10)\n",
    "        \n",
    "\n",
    "        # Divide the test set into a validation set and final test set.\n",
    "        self.X_validation, self.X_test, self.T_validation, self.T_test = cross_validation.train_test_split(self.X_test, \n",
    "                                                                                       self.T_test,\n",
    "                                                                                       test_size=.5)\n",
    "        pass\n",
    "    \n",
    "    def neural_network_functions(self):        \n",
    "        \"\"\"\n",
    "        it contains non-linear functions:\n",
    "        1.) activation functions: sigmoid, softmax, \n",
    "        2.) cost function: cross_entropy\n",
    "        3.) gradients : sigmoid, \n",
    "        4.) learning rule: weight update(delta_w)\n",
    "        \"\"\"        \n",
    "        #sigmoid\n",
    "        self.sigmoid = lambda x: scipy.special.expit(x)                \n",
    "        #softmax\n",
    "        self.softmax = lambda z : np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "        #another cross_entropy or cost function\n",
    "        self.cross_entropy_loss = lambda y, t: - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))    \n",
    "        #gradients\n",
    "        self.gradient = lambda w, x, t: (self.nn(x, w) - t).T * x\n",
    "        #sigmoid derivative                \n",
    "        self.sigmoid_gradient = lambda y: np.multiply(y, (1 - y))\n",
    "        #update delta w\n",
    "        self.delta_w = lambda w_k, x, t, learning_rate: learning_rate * self.gradient(w_k, x, t)                \n",
    "        pass    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Training of model involves two steps:\n",
    "    #1.) forward propagation\n",
    "    #2.) backward propagation\n",
    "    def forward_propagation_step(self, inputs, list_of_layers):\n",
    "        \"\"\"\n",
    "        Forward propagation: it is used to calculate activation of nth_layer and \n",
    "        eventually calculates the output values from the model(or model) in the \n",
    "        output layer\n",
    "        \"\"\"\n",
    "        n_layer_activations = [inputs] #store activation values in a list\n",
    "        x = inputs \n",
    "        for nth_layer in list_of_layers:\n",
    "            y = nth_layer.get_output(x) # get output from nth-layer\n",
    "            n_layer_activations.append(y) # save nth-layer output in activation list        \n",
    "            x = n_layer_activations[-1] # get the last layer activation values and make it input for next layer\n",
    "            pass\n",
    "        return n_layer_activations\n",
    "\n",
    "    def back_propagation_step(self, list_of_activations, targets, list_of_layers):\n",
    "        \"\"\"\n",
    "        Backpropagation step:\n",
    "        it involves updating the parameter in the model\n",
    "        \"\"\"\n",
    "        param_grads = collections.deque()\n",
    "        output_grad = None\n",
    "\n",
    "        for nth_last_layer in reversed(list_of_layers): # access every layer from the last layer\n",
    "            y = list_of_activations.pop()  # get activations from last layer and delete the last layer \n",
    "                                           # from the list\n",
    "            if output_grad is None: # if there is no gradient in the output layer\n",
    "                input_grad = nth_last_layer.get_input_grad(y, targets)\n",
    "                pass\n",
    "            else :\n",
    "                input_grad = nth_last_layer.get_input_grad(y, output_grad)\n",
    "                pass\n",
    "\n",
    "            x = list_of_activations[-1] # get activation from the last layer \n",
    "            grads = nth_last_layer.get_params_grad(x, output_grad) # get parameters of gradient \n",
    "                                                                   # between nth and (n-1)th layer\n",
    "            param_grads.appendleft(grads) # make a list of parameter gradients\n",
    "\n",
    "            output_grad = input_grad      \n",
    "\n",
    "        return list(param_grads) # return list of parameters\n",
    "    \n",
    "    # Define a method to update the parameters\n",
    "    def delta_weight(self, list_of_layers, param_grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Function to update the parameters of the given layers with the given gradients\n",
    "        by gradient descent with the given learning rate.      \n",
    "        \"\"\"     \n",
    "        for layer, layer_backprop_grads in zip(list_of_layers, param_grads):        \n",
    "            for param, grad in zip(layer.get_params_iter(), layer_backprop_grads): #access each parameter and gradient\n",
    "                param -= learning_rate * grad # Update each parameter using gradient descent\n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    \n",
    "    def train(self, X_train, T_train):        \n",
    "        \"\"\"\n",
    "        Perform training of data using forward and back propagation functions \n",
    "        given above and store the training and validation costs in every epoch\n",
    "        for future analsis(for plotting the graphs)\n",
    "        \"\"\"        \n",
    "        training_costs = [] # initialize list to store the cost of training\n",
    "        validation_costs = [] # initialize list to store the cost of validation \n",
    "\n",
    "        epochs = 300  # initialize maximum number of epochs for training\n",
    "        learning_rate = 0.15  # learning rate for gradient descent\n",
    "\n",
    "        print (\"waiting for the model to be trained ...\")\n",
    "        # Construct an array of data source\n",
    "        ds = data_source.ArrayDataSource([X_train, T_train]) # use training data and its target labels        \n",
    "        for epoch in range(epochs): # Training with maximum number of epochs\n",
    "            # Iterate over samples and uses batches of 25 in a random order\n",
    "            for X_train_data, T_label in ds.batch_iterator(batch_size=25, shuffle=np.random.RandomState(25)):   \n",
    "                activations = self.forward_propagation_step(X_train_data, list_of_layers)  # Get activations\n",
    "                param_grads = self.back_propagation_step(activations, T_label, list_of_layers)  # Get gradients\n",
    "                self.delta_weight(list_of_layers, param_grads, learning_rate)  # Update  parameters\n",
    "\n",
    "            # Get full training cost in every epoch and store every cost in a list\n",
    "            activations = self.forward_propagation_step(X_train, list_of_layers)\n",
    "            train_cost = list_of_layers[-1].get_cost(activations[-1], T_train)\n",
    "            training_costs.append(train_cost) \n",
    "            # Get full validation cost in every epoch and store every cost in a list\n",
    "            activations = self.forward_propagation_step(self.X_validation, list_of_layers)\n",
    "            validation_cost = list_of_layers[-1].get_cost(activations[-1], self.T_validation)\n",
    "            validation_costs.append(validation_cost)\n",
    "\n",
    "            if len(validation_costs) > 3: # Stop training if the cost on the validation \n",
    "                                          # set doesn't decrease for 3 iterations\n",
    "                if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "                    break\n",
    "\n",
    "        nb_of_epochs = epoch + 1  # total number of epochs that have been executed\n",
    "        print (\"Model trained !\")\n",
    "        return nb_of_epochs, training_costs, validation_costs \n",
    "\n",
    "    def test_accuracy(self, X_Test, list_of_layers):   \n",
    "        \"\"\"\n",
    "        Get the test accuracy of the test data on the trained model\n",
    "        \"\"\"\n",
    "        y_true = np.argmax(ANN.T_test, axis=1)  # Get the target outputs\n",
    "        activations = ANN.forward_propagation_step(ANN.X_test, list_of_layers)  # Get activation of test samples\n",
    "        y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "        test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Get test set accuracy\n",
    "        print('The accuracy on the test set is {:.2f}'.format(test_accuracy))\n",
    "        pass\n",
    "    \n",
    "    def plot_cost_vs_epochs(self, nb_of_epochs, training_costs, validation_costs):\n",
    "        \"\"\"\n",
    "        Evaluate the performce of the model by plotting the graph between \n",
    "        cost and epochs\n",
    "        \"\"\"\n",
    "        # Plot full training set, and validation costs\n",
    "        epoch_x_inds = np.linspace(1, nb_of_epochs, num=nb_of_epochs)\n",
    "        # Plot the cost over the iterations\n",
    "        plt.plot(epoch_x_inds, training_costs, 'r-', linewidth=3, label='cost full training set')\n",
    "        # Add labels to the plot\n",
    "        plt.xlabel('Epochs --->')\n",
    "        plt.ylabel('$Error --->$', fontsize=15)\n",
    "        plt.title('cost vs epochs')\n",
    "        plt.legend()\n",
    "        x1,x2,y1,y2 = plt.axis()\n",
    "        plt.axis((0,nb_of_epochs,0,2.5))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our application we need three different kinds of layers:\n",
    "# 1.) Linear layer\n",
    "# 2.) Singmoid/Logistic layer\n",
    "# 3.) Softmax layer\n",
    "# for these three layers we need a base class, which\n",
    "# can share some function among the three given layer\n",
    "\n",
    "\n",
    "# Base class with common methods among the layers\n",
    "# used in the model\n",
    "class BaseLayer(object):\n",
    "    \"\"\"\n",
    "    Base class for three different layers:\n",
    "    This class contains some basic methods which \n",
    "    are used among the three different types of layers \n",
    "    used in the given model of neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"\n",
    "        Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"\n",
    "        Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        1.) \"X\" is the input.\n",
    "        2.) \"output_grad\" is the gradient at the output of the given layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        performs linear transformation in the feed forward step.\n",
    "        and it will act as an input to the next layer(or to the\n",
    "        activation layer).\n",
    "        1.) \"X\" is the input.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"\n",
    "        During backpropagation, it returns the gradient\n",
    "        of the input of the given given layer.        \n",
    "        1.) \"Y\" is the pre-computed output of the given layer\n",
    "        during forward propagation.         \n",
    "        2.) For the output layer instead of using \"Y\", output\n",
    "        layer uses the Target \"T\" to compute the gradient \n",
    "        instead of using the output_grad.        \n",
    "        3.) \"output_grad\" is the gradient at the output of the given layer\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    \"Linear Layer\" performs a linear transformation to its input.\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"    \n",
    "    def __init__(self, \n",
    "                 no_of_input_nodes, \n",
    "                 no_of_output_nodes):\n",
    "        # linear layers initializes the weights and bias \n",
    "        # between the two consecutive layers whereas as rest\n",
    "        # of the layers are responsible for the computation \n",
    "        # of non-linear (or activation functions) functions.        \n",
    "        self.W = np.random.randn(no_of_input_nodes,\n",
    "                                 no_of_output_nodes) * 0.1     \n",
    "            \n",
    "        self.b = np.zeros(no_of_output_nodes) # bias initialized with zero\n",
    "        pass\n",
    "        \n",
    "    # forward propagation step\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Performs the linear transformation for the forward \n",
    "        step which will act as an input to the activation layer.\n",
    "        \"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "    \n",
    "    # forward propagation step\n",
    "    def get_params_iter(self):\n",
    "        \"\"\"\n",
    "        Return an iterator over the parameters.\n",
    "        \"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"\n",
    "        During the backpropagation it returns a list \n",
    "        of gradients over the parameters(or weight and\n",
    "        parameters)\n",
    "        \"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"\n",
    "        Returns the gradient(or error) at the inputs of\n",
    "        the given layer.\n",
    "        \"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic layer\n",
    "class SigmoidLayer(BaseLayer, NeuralNetwork):\n",
    "    \"\"\"\n",
    "    \"Logistic Layer\": computes the logistic or sigmoid\n",
    "    function from the output of the \"Linear Layer\"\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward propagation step:\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward step transformation.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(X)\n",
    "    \n",
    "    # backpropagation step\n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"\n",
    "        Return the gradient at the inputs of this layer.\n",
    "        \"\"\"\n",
    "        return np.multiply(self.sigmoid_gradient(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax layer\n",
    "class SoftmaxLayer(BaseLayer, NeuralNetwork):\n",
    "    \"\"\"\n",
    "    The softmax output layer computes the classification propabilities at the output.\n",
    "    Please note that every layer has two steps:\n",
    "    1.) forward propagation: involves calculating linear transformation, \n",
    "    activation functions    \n",
    "    2.) backprogation: involves parameter updating(or weight updating)\n",
    "    calculating gradient or error at the output of the given layer\n",
    "    \"\"\"\n",
    "    #forward propagation\n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward step transformation.\n",
    "        \"\"\"\n",
    "        return self.softmax(X)\n",
    "    \n",
    "    #backward propagation\n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"\n",
    "        Return the gradient at the inputs of this layer.\n",
    "        \"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"\n",
    "        Cross-Entropy cost function\n",
    "        Return the cost at the output of this output layer.\n",
    "        \"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for the model to be trained ...\n",
      "Model trained !\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 1.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    \"\"\"\n",
    "    Create and train a model\n",
    "    sample of model with layers is :\n",
    "    input layer -> linear layer -> sigmoid layer-> linear layer -> sigmoid layer -> linear layer -> softmax layer\n",
    "    \"\"\"\n",
    "    #initialize neural network class\n",
    "    ANN = NeuralNetwork()\n",
    "    \n",
    "    # add nodes(or neurons) in the given layer,\n",
    "    no_of_hidden_nodes_layer_1 = 25 # hidden layer 1  \n",
    "    no_of_hidden_nodes_layer_2 = 15 # hidden layer 2  (# not required)\n",
    "    no_of_hidden_nodes_layer_3 = 20 # hidden layer 3  (# not required)\n",
    "\n",
    "    list_of_layers = [] # store layers in a list\n",
    "\n",
    "    # Add first hidden layer\n",
    "    list_of_layers.append(LinearLayer(ANN.X_train.shape[1], no_of_hidden_nodes_layer_1))\n",
    "    list_of_layers.append(SigmoidLayer())\n",
    "    \n",
    "    #list_of_layers.append(LinearLayer(no_of_hidden_nodes_layer_1, no_of_hidden_nodes_layer_2))\n",
    "    #list_of_layers.append(SigmoidLayer())    \n",
    "    \"\"\"\n",
    "    Here you can add \"N\" number of layers exactly in the same way as above......\n",
    "    Examples are :\n",
    "    # list_of_layers.append(LinearLayer(no_of_nodes_in_nth_hidden_layer, \n",
    "                                        no_of_nodes_in_nth+1_hidden_layer))\n",
    "    # list_of_layers.append(SigmoidLayer())                                        \n",
    "    \"\"\"\n",
    "    list_of_layers.append(LinearLayer(no_of_hidden_nodes_layer_1, ANN.T_train.shape[1]))\n",
    "    list_of_layers.append(SoftmaxLayer()) # output layer\n",
    "        \n",
    "    #after model creation, we need to train the model.\n",
    "    epochs, training_costs, validation_costs = ANN.train(ANN.X_train, ANN.T_train)  # training of model and return\n",
    "                                                                                    # epochs, training, validation costs\n",
    "    ANN.plot_cost_vs_epochs(epochs, training_costs, validation_costs) #plot graph between training, validation costs and epochs\n",
    "    ANN.test_accuracy(ANN.T_test, list_of_layers) # get accuracy on test data\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
